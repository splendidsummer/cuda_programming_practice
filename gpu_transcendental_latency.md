# 为什么 GPU 上超越函数（exp / log / sin / cos）具有高延迟？

## 1. 什么是“超越函数”（transcendental）

在 GPU 中，一般称为 **transcendental / special functions**，包括：

- `exp`, `log`, `sin`, `cos`, `tan`, `atan2`, `pow` …
- CUDA 中对应：`expf`, `logf`, `sinf`, `cosf`，以及 `__expf` 等快速近似 intrinsic。

对比对象：

- **低延迟基础运算**：加、减、乘、FMA（乘加）、简单比较，这些都在 FP32/INT32 核上高速执行。

---

## 2. 为什么它们“天生就慢”（高延迟）

### **（1）数学行为复杂：硬件不能一步完成**

- 加减乘：可以用电路一步完成。
- `exp(x)`, `log(x)`, `sin(x)` 等：
  - 依赖**逼近算法**：多项式逼近（如 Chebyshev）、CORDIC、查表 + 插值。
  - 需要多次乘加、区间缩放、修正等多步操作。

GPU 会将这些微操作封装为单指令（如 MUFU），但内部仍需很多 cycle。

你可以把它理解为：

- **加法 = 一步操作**
- **exp(x) = 变换区间 → 查表 → 多次乘加 → 尾数修正**  
  → 自然更慢

---

### **（2）SFU 数量极少：资源成为瓶颈**

GPU 中存在 **SFU（Special Function Unit）** 专门负责超越函数。

- 每个 SM 里：  
  - FP32 Core 数量巨大  
  - **SFU 却只有很少几个**

结果：

- 超越函数每条指令本来就多 cycle
- 还要和其他 warp **抢 SFU**
- 于是出现 **高延迟 + 低吞吐**

---

### **（3）精度要求导致耗时增加**

默认 `expf/logf/sinf` 需要满足 IEEE 精度：

- 必须控制 ulp 级误差
- 需要更多分段、更多多项式阶数、更复杂的修正

CUDA 提供两套：

| 函数 | 特点 |
|------|------|
| `expf()` | 高精度，高延迟 |
| `__expf()` | 低精度，低延迟，吞吐更高 |

开启 `-use_fast_math` 后 kernel 会走快速路径，因此延迟明显下降。

---

### **（4）GPU 设计哲学：靠并行掩盖，而不是降低单次延迟**

GPU 的目标不是让每条指令延迟极低，而是：

- **靠大量 warp 互相切换来掩盖等待**

例如：

- warp A 正在等待 `exp(x)`  
- warp 调度器切换去运行 warp B 的其他指令

对于加乘等便宜操作：

- 延迟低 + 吞吐高

但对于 transcendental：

- 少量 SFU + 多 cycle  
- 能藏就藏，藏不住时就直接暴露“高延迟”

---

## 3. 为什么在 GPU 上“体感更明显”？

原因包括：

### **（1）基础算子太快，反差巨大**
比如 FMA 每拍都能跑，于是和 exp 一比：
- 延迟差距显得非常夸张。

### **（2）SFU 很容易饱和**
kernel 中若频繁调用 `exp/log`：
- Nsight 里 SFU 饱和  
- warp 全在等超越函数

### **（3）数据依赖链导致延迟无法掩盖**

例如：

```cpp
float y = expf(x);
float z = y * w + b;
```

如果没有其他可并行的 warp，那么延迟会完全暴露。

---

## 4. 实战中如何减少高延迟损害？

### **（1）减少调用次数**
- 对常量的 `exp(constant)` 提前算好，存成常量。

---

### **（2）使用 fast math / 近似版本**

CUDA C++：

- `expf → __expf`
- `logf → __logf`
- 编译加 `-use_fast_math`

PyTorch：

- 使用更便宜的近似激活，如 `gelu_tanh`

---

### **（3）用便宜运算替代复杂超越函数**

例如：

- `Swish(x) = x * sigmoid(x)` （内部有 exp）可用更简单激活替代
- softmax 中减少 exp 调用次数（合并 softmax 或更小维度）

---

### **（4）提高并行度掩盖 SFU 延迟**

- grid/block 规模足够大
- 让 warp 调度器能切换更多独立工作
- 避免串联多次依赖的 `exp/log`

---

## 5. 一句话总结

> **超越函数在 GPU 上之所以高延迟，是因为数学本身复杂，需要多步逼近，在硬件中通过少量 SFU 完成，多 cycle、易拥塞，因此明显比加减乘慢很多。**

如果你愿意贴出你的 kernel 或 Nsight Compute 截图，我可以帮你定位：
- 是 **SFU 饱和**  
- 还是 **warp 不够多** 导致延迟暴露。
